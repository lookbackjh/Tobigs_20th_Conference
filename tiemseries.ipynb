{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본적인 전처리 함수들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader용 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightning in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (2.0.9.post0)\n",
      "Requirement already satisfied: Jinja2<5.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (3.1.2)\n",
      "Requirement already satisfied: PyYAML<8.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (6.0.1)\n",
      "Requirement already satisfied: arrow<3.0,>=1.2.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (1.3.0)\n",
      "Requirement already satisfied: backoff<4.0,>=2.2.1 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (2.2.1)\n",
      "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (4.12.2)\n",
      "Requirement already satisfied: click<10.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (8.1.7)\n",
      "Requirement already satisfied: croniter<1.5.0,>=1.3.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (1.4.1)\n",
      "Requirement already satisfied: dateutils<2.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (0.6.12)\n",
      "Requirement already satisfied: deepdiff<8.0,>=5.7.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (6.6.0)\n",
      "Requirement already satisfied: fastapi<2.0,>=0.92.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (0.103.2)\n",
      "Requirement already satisfied: fsspec<2025.0,>=2022.5.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (2023.9.2)\n",
      "Requirement already satisfied: inquirer<5.0,>=2.10.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (3.1.3)\n",
      "Requirement already satisfied: lightning-cloud>=0.5.38 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (0.5.39)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.7.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (0.9.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (1.26.0)\n",
      "Requirement already satisfied: packaging in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (23.2)\n",
      "Requirement already satisfied: psutil<7.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (5.9.5)\n",
      "Requirement already satisfied: pydantic<2.2.0,>=1.7.4 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (2.1.1)\n",
      "Requirement already satisfied: python-multipart<2.0,>=0.0.5 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (0.0.6)\n",
      "Requirement already satisfied: requests<4.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (2.31.0)\n",
      "Requirement already satisfied: rich<15.0,>=12.3.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (13.6.0)\n",
      "Requirement already satisfied: starlette in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (0.27.0)\n",
      "Requirement already satisfied: starsessions<2.0,>=1.2.1 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (1.3.0)\n",
      "Requirement already satisfied: torch<4.0,>=1.11.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (2.1.0)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (1.2.0)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (4.66.1)\n",
      "Requirement already satisfied: traitlets<7.0,>=5.3.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (5.11.2)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (4.8.0)\n",
      "Requirement already satisfied: urllib3<4.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (2.0.6)\n",
      "Requirement already satisfied: uvicorn<2.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (0.23.2)\n",
      "Requirement already satisfied: websocket-client<3.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (1.6.4)\n",
      "Requirement already satisfied: websockets<13.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (11.0.3)\n",
      "Requirement already satisfied: pytorch-lightning in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning) (2.0.9.post0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from arrow<3.0,>=1.2.0->lightning) (2.8.2)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from arrow<3.0,>=1.2.0->lightning) (2.8.19.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from beautifulsoup4<6.0,>=4.8.0->lightning) (2.5)\n",
      "Requirement already satisfied: pytz in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from dateutils<2.0->lightning) (2023.3.post1)\n",
      "Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from deepdiff<8.0,>=5.7.0->lightning) (4.1.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from fastapi<2.0,>=0.92.0->lightning) (3.7.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from fsspec[http]<2025.0,>2021.06.0->lightning) (3.8.6)\n",
      "Requirement already satisfied: blessed>=1.19.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from inquirer<5.0,>=2.10.0->lightning) (1.20.0)\n",
      "Requirement already satisfied: python-editor>=1.0.4 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from inquirer<5.0,>=2.10.0->lightning) (1.0.4)\n",
      "Requirement already satisfied: readchar>=3.0.6 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from inquirer<5.0,>=2.10.0->lightning) (4.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from Jinja2<5.0->lightning) (2.1.3)\n",
      "Requirement already satisfied: pyjwt in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning-cloud>=0.5.38->lightning) (2.8.0)\n",
      "Requirement already satisfied: six in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from lightning-cloud>=0.5.38->lightning) (1.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from pydantic<2.2.0,>=1.7.4->lightning) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.4.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from pydantic<2.2.0,>=1.7.4->lightning) (2.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from requests<4.0->lightning) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from requests<4.0->lightning) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from requests<4.0->lightning) (2023.7.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from rich<15.0,>=12.3.0->lightning) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from rich<15.0,>=12.3.0->lightning) (2.16.1)\n",
      "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from starsessions<2.0,>=1.2.1->lightning) (2.1.2)\n",
      "Requirement already satisfied: filelock in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (3.12.4)\n",
      "Requirement already satisfied: sympy in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=1.11.0->lightning) (12.2.140)\n",
      "Requirement already satisfied: h11>=0.8 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from uvicorn<2.0->lightning) (0.14.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi<2.0,>=0.92.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi<2.0,>=0.92.0->lightning) (1.1.3)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (0.2.8)\n",
      "Requirement already satisfied: mdurl~=0.1 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<15.0,>=12.3.0->lightning) (0.1.2)\n",
      "Requirement already satisfied: setuptools>=41.0 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning) (68.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.11.0->lightning) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lightning\n",
    "# 코렙용\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminer 훈련용 데이터셋, 시계열 예측용 데이터셋을 위한 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Discriminative2_Dataset(Dataset):\n",
    "    def __init__(self, source,target,source_y,target_y, source_label,target_label):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.len = len(source)\n",
    "        self.source_y=source_y\n",
    "        self.target_y=target_y\n",
    "        self.source_label=source_label\n",
    "        self.target_label=target_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src=torch.tensor(self.source[idx],dtype=torch.float32)\n",
    "        tgt=torch.tensor(self.target[idx],dtype=torch.float32)\n",
    "        src_y=torch.tensor(self.source_y[idx],dtype=torch.float32)\n",
    "        tgt_y=torch.tensor(self.target_y[idx],dtype=torch.float32)\n",
    "        src_label=torch.tensor(self.source_label[idx],dtype=torch.float32)\n",
    "        tgt_label=torch.tensor(self.target_label[idx],dtype=torch.float32)\n",
    "\n",
    "        return src,tgt,src_y,tgt_y,src_label,tgt_label\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minmax scaling, 대분류, 소분류 카테고리 불러오는 함수 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# lightning\n",
    "import pytorch_lightning as pl\n",
    "# minmax scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def large_categories_getter(source_idx,target_idx):\n",
    "    a=pd.read_csv('data/train.csv')\n",
    "    largeones=a['대분류'].unique()\n",
    "    smallones=a['소분류'].unique()\n",
    "    # find row where 대분류 is largeones[0]\n",
    "    largerows=a.loc[(a['대분류'] == largeones[source_idx])] \n",
    "    targets=largerows['소분류'].unique()\n",
    "\n",
    "\n",
    "    targetrows=largerows.loc[(largerows['소분류'] == targets[target_idx])]\n",
    "    # sum every row\n",
    "    targetrows.drop(['ID','대분류','중분류','소분류','브랜드','제품'],axis=1,inplace=True) \n",
    "    targetrows=targetrows.sum(axis=0)\n",
    "\n",
    "    # sum every row\n",
    "    largerows.drop(['ID','대분류','중분류','소분류','브랜드','제품'],axis=1,inplace=True) \n",
    "    sourcerows=largerows.sum(axis=0)\n",
    "    return sourcerows,targetrows\n",
    "\n",
    "\n",
    "def small_categories_getter():\n",
    "\n",
    "    a=pd.read_csv('data/train.csv')\n",
    "    largeones=a['대분류'].unique()\n",
    "    smallones=a['소분류'].unique()\n",
    "    # find row where 대분류 is largeones[0]\n",
    "    smallrows=a.loc[(a['소분류'] == smallones[0])] \n",
    "    # sum every row\n",
    "    smallrows.drop(['ID','대분류','중분류','소분류','브랜드','제품'],axis=1,inplace=True) \n",
    "    totalsmallrows=smallrows.sum(axis=0)\n",
    "    return totalsmallrows\n",
    "\n",
    "def create_sequence(df, seq_length,pred_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(df)-seq_length-pred_length):\n",
    "        x = df[i:(i+seq_length)]\n",
    "        y = df[(i+seq_length):(i+seq_length+pred_length)]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "\n",
    "def preprocessing(xs,ys):\n",
    "    ## train test split\n",
    "    train_len=int(len(xs)*0.7)\n",
    "    train_x=xs[:train_len]\n",
    "    train_y=ys[:train_len]\n",
    "    test_x=xs[train_len:]\n",
    "    test_y=ys[train_len:]\n",
    "\n",
    "    # min max scaler\n",
    "    x_scaler=MinMaxScaler()\n",
    "    y_scaler=MinMaxScaler()\n",
    "    train_x=x_scaler.fit_transform(train_x)\n",
    "    train_y=y_scaler.fit_transform(train_y)\n",
    "    test_x=x_scaler.transform(test_x)\n",
    "    test_y=y_scaler.transform(test_y)\n",
    "\n",
    "    # train test split\n",
    "    return train_x,train_y,test_x,test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting 모델 정의 (Source 훈련용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = nn.LSTM(self.input_dim, self.hidden_dim, batch_first=True) #used lstm 그러나 CNN, LInear 등 다른 모형도 가능\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.unsqueeze(2)\n",
    "        # x shape: (batch_size, seq_length, input_dim) , input_dim=1 단변량이니까\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(self.device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_dim).to(self.device)\n",
    "        out_encoder, (hn, cn) = self.encoder(x, (h0, c0))\n",
    "        # 우리가 필요한건 마지막 시점에서의 hidden state\n",
    "        return out_encoder\n",
    "\n",
    "\n",
    "class Forecaster(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #self.latent_dim = latent_dim\n",
    "        self.encoder = Encoder(input_dim, hidden_dim)\n",
    "        self.forecaster=nn.Sequential(\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim,output_dim)\n",
    "        )\n",
    "        ## 32-> 5\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x=x.unsqueeze(2) # shape: (batch_size, seq_length, input_dim) 32, 30, 1(단변량이니ㄱ까)\n",
    "        out_encoder=self.encoder.forward(x)\n",
    "        out=self.forecaster(out_encoder[:,-1,:]) # 우리가 필요한건 마지막 시점에서의 hidden state이니까 out_encoder[:,-1,:]을 해줌\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch # 0~30-> 31~35\n",
    "        y_hat=self.forward(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "    \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discriminator 모델 정의(target 훈련용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# lightening\n",
    "import pytorch_lightning as pl\n",
    "class Discriminator2(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, source_encoder,target_encoder,hidden_dim,latent_dim,input_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.source_encoder = source_encoder\n",
    "        self.target_encoder = target_encoder\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_dim=output_dim\n",
    "        self.automatic_optimization = False\n",
    "        self.input_dim=input_dim\n",
    "        self.sig=nn.Sigmoid()\n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, 1),\n",
    "        )\n",
    "        self.reconstructer=nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.input_dim),\n",
    "        )\n",
    "        self.forecaster=nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, s,t):\n",
    "        z_source = self.source_encoder(s)\n",
    "        z_source=z_source[:,-1,:]\n",
    "        z_target = self.target_encoder(t)\n",
    "        z_target=z_target[:,-1,:]\n",
    "\n",
    "        z = torch.cat((z_source, z_target), dim=0)\n",
    "        shuffle_idx = torch.randperm(z.size(0))\n",
    "        z = z[shuffle_idx]\n",
    "        return self.discriminator(z.detach())\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        source,target, source_y,target_y,source_label,target_label = batch # x,y refers to sequence ang label(0 or 1)\n",
    "        optimizer_rs,optimizer_rt,optimizer_d=self.optimizers() ## ligthning 내장함수\n",
    "\n",
    "        # reconstruction source loss\n",
    "        optimizer_rs.zero_grad()\n",
    "        optimizer_rt.zero_grad()\n",
    "        z_source=self.source_encoder(source)\n",
    "        z_source=z_source[:,-1,:]\n",
    "        \n",
    "        \n",
    "        z_target=self.target_encoder(target)\n",
    "        z_target=z_target[:,-1,:]\n",
    "\n",
    "\n",
    "\n",
    "        ## 1. get reconstruction source loss\n",
    "        source_reconstruction=self.reconstructer(z_source)\n",
    "        source_reconstruction_loss=nn.MSELoss()(source_reconstruction,source)\n",
    "\n",
    "        source_forecast=self.forecaster(z_source)\n",
    "        source_forecast_loss=nn.MSELoss()(source_forecast,source_y)\n",
    "\n",
    "        ## 2. get reconstruction target loss\n",
    "        target_reconstruction=self.reconstructer(z_target)\n",
    "        target_reconstruction_loss=nn.MSELoss()(target_reconstruction,target)\n",
    "\n",
    "        target_forecast=self.forecaster(z_target)\n",
    "        target_forecast_loss=nn.MSELoss()(target_forecast,target_y)\n",
    "\n",
    "        rec_loss_total=source_reconstruction_loss+source_forecast_loss+target_reconstruction_loss+target_forecast_loss\n",
    "        self.manual_backward(rec_loss_total)\n",
    "        optimizer_rs.step()\n",
    "        optimizer_rt.step()\n",
    "\n",
    "        ## 3. get Discriminative loss\n",
    "        optimizer_d.zero_grad()\n",
    "        z = self.forward(source,target)\n",
    "        z=z.squeeze()\n",
    "        y=torch.cat((source_label,target_label),dim=0)\n",
    "        d_loss = nn.BCEWithLogitsLoss()(z, y)\n",
    "        self.manual_backward(d_loss)\n",
    "        optimizer_d.step()\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt_rs=torch.optim.Adam(self.source_encoder.parameters(), lr=1e-4)\n",
    "        opt_rt=torch.optim.Adam(self.target_encoder.parameters(), lr=1e-4)\n",
    "        opt_d=torch.optim.Adam(self.discriminator.parameters(), lr=1e-4)\n",
    "        opt_t=torch.optim.Adam(self.target_encoder.parameters(), lr=1e-4)\n",
    "\n",
    "        return [opt_rs,opt_rt,opt_d],[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델  훈련 및 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_243907/3715573595.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  targetrows.drop(['ID','대분류','중분류','소분류','브랜드','제품'],axis=1,inplace=True)\n",
      "/tmp/ipykernel_243907/3715573595.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  largerows.drop(['ID','대분류','중분류','소분류','브랜드','제품'],axis=1,inplace=True)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:69: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Encoder    | 17.2 K\n",
      "1 | forecaster | Sequential | 5.5 K \n",
      "------------------------------------------\n",
      "22.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "22.6 K    Total params\n",
      "0.090     Total estimated model params size (MB)\n",
      "/workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/workspaces/jjh-gpu1-temp/IDSL/NSFMR/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|██████████| 8/8 [00:00<00:00, 311.21it/s, v_num=14, train_loss_step=0.00228, train_loss_epoch=0.00296]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|██████████| 8/8 [00:00<00:00, 278.69it/s, v_num=14, train_loss_step=0.00228, train_loss_epoch=0.00296]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | source_encoder | Encoder    | 17.2 K\n",
      "1 | target_encoder | Encoder    | 17.2 K\n",
      "2 | sig            | Sigmoid    | 0     \n",
      "3 | discriminator  | Sequential | 4.2 K \n",
      "4 | reconstructer  | Sequential | 10.7 K\n",
      "5 | forecaster     | Sequential | 5.5 K \n",
      "----------------------------------------------\n",
      "54.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "54.6 K    Total params\n",
      "0.219     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|██████████| 8/8 [00:00<00:00, 154.29it/s, v_num=15]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|██████████| 8/8 [00:00<00:00, 143.36it/s, v_num=15]\n",
      "Test RMSE using source only: 0.428\n",
      "Test RMSE using target encoder trained: 0.389\n"
     ]
    }
   ],
   "source": [
    "source_idx=0  # 대분류 총 5개 , 0,1,2,3,4 선택 가능. \n",
    "target_idx=4 # 각 대분류마다 할당된 소분류-> 조금씩 다름\n",
    "source_data,target_data=large_categories_getter(source_idx,target_idx)\n",
    "#target_data,small_data=large_categories_getter(target_idx,0)\n",
    "\n",
    "\n",
    "## 2. 데이터 전처리\n",
    "seq_length=100 # 얼마의 기간을 가지고 다음 기간을 예측할 것인가\n",
    "pred_length=20 # 다음 기간을 얼마나 예측할 것인가 ex) 다음 5일에 대한 예측치를 한번에 제공\n",
    "xs,ys=create_sequence(source_data,seq_length,pred_length)\n",
    "xt,yt=create_sequence(target_data,seq_length,pred_length)\n",
    "\n",
    "# 전처리\n",
    "train_xs,train_ys,test_xs,test_ys=preprocessing(xs,ys) # train test split, min max scaler    \n",
    "train_xt,train_yt,test_xt,test_yt=preprocessing(xt,yt) # train test split, min max scaler\n",
    "\n",
    "train_s_label=np.ones(len(train_xs))\n",
    "train_t_label=np.zeros(len(train_xt))\n",
    "\n",
    "discriminative_dataset=Discriminative2_Dataset(train_xs,train_xt,train_ys,train_yt,train_s_label,train_t_label)\n",
    "discriminative_loader=DataLoader(discriminative_dataset,batch_size=32,shuffle=True)\n",
    "\n",
    "# dataset, dataloader\n",
    "train_dataset_source=TimeSeriesDataset(train_xs,train_ys)\n",
    "test_dataset_source=TimeSeriesDataset(test_xs,test_ys)\n",
    "train_loader_source=DataLoader(train_dataset_source,batch_size=32,shuffle=True)\n",
    "test_loader_sourcer=DataLoader(test_dataset_source,batch_size=32,shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ADDA 1. Source 모델 훈련시키기\n",
    "source_model = Forecaster(input_dim=1, hidden_dim=64,output_dim=pred_length) #LSTM Encoder\n",
    "source_trainer=pl.Trainer(max_epochs=300)\n",
    "source_trainer.fit(source_model,train_loader_source,test_loader_sourcer)\n",
    "\n",
    "\n",
    "\n",
    "## ADDA 2. Discriminator 훈련, Target Encoder 훈련 (번갈아 가면서)\n",
    "target_encoder=Encoder(input_dim=1, hidden_dim=64)\n",
    "discriminator=Discriminator2(source_model.encoder,target_encoder, hidden_dim=64, latent_dim=64,input_dim=seq_length,output_dim=pred_length)\n",
    "discriminator_trainer=pl.Trainer(max_epochs=200)\n",
    "discriminator_trainer.fit(discriminator,discriminative_loader)\n",
    "\n",
    "## ADDA 3. Evaluation\n",
    "# 1) Source 모델 평가\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "source_model.eval()\n",
    "source_model=source_model.to(device)\n",
    "\n",
    "test_xt=torch.tensor(test_xt,dtype=torch.float32).to(device)\n",
    "#test_xt=test_xt.unsqueeze(2)\n",
    "source_output=source_model.forward(test_xt)\n",
    "source_output=source_output.squeeze()\n",
    "source_output=source_output.detach().cpu().numpy()\n",
    "# evaluate rmse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse = sqrt(mean_squared_error(test_yt[:,], source_output[:,]))\n",
    "print('Test RMSE using source only: %.3f' % rmse)\n",
    "\n",
    "\n",
    "# 2) Target 모델 평가\n",
    "target_encoder.eval()\n",
    "target_encoder=target_encoder.to(device)\n",
    "target_output=target_encoder.forward(test_xt)\n",
    "tar_pred=source_model.forecaster(target_output[:,-1,:])\n",
    "tar_pred=tar_pred.squeeze()\n",
    "tar_pred=tar_pred.detach().cpu().numpy()\n",
    "# evaluate rmse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse = sqrt(mean_squared_error(test_yt[:,], tar_pred[:,]))\n",
    "print('Test RMSE using target encoder trained: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 20)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_xt.shape\n",
    "source_output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
